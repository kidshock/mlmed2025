{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-30T01:10:10.280574Z","iopub.status.busy":"2025-03-30T01:10:10.280391Z","iopub.status.idle":"2025-03-30T01:10:37.737001Z","shell.execute_reply":"2025-03-30T01:10:37.736226Z","shell.execute_reply.started":"2025-03-30T01:10:10.280555Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/inme-veri-seti-stroke-dataset 0 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ 2 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/İnme Yok 4551 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/Kanama 0 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/Kanama/DICOM 1093 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/Kanama/OVERLAY 1093 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/Kanama/PNG 1093 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum 0 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum/DICOM 100 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum/OVERLAY 100 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum/MASKS 100 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum/PNG 97 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/İskemi 0 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/İskemi/DICOM 1130 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/İskemi/OVERLAY 1130 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/İskemi/PNG 1130 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_1_Oturum 1 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_1_Oturum/OVERLAY 200 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_1_Oturum/MASKS 200 files\n","/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_1_Oturum/PNG 200 files\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","#for dirname, _, filenames in os.walk('/kaggle/input/inme-veri-seti-stroke-dataset'):\n","\n","data_dir_0 = '/kaggle/input/inme-veri-seti-stroke-dataset'\n","for root, dirs, files in os.walk(data_dir_0):\n","    print(root, len(files), 'files')\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:11:21.992595Z","iopub.status.busy":"2025-03-30T01:11:21.992296Z","iopub.status.idle":"2025-03-30T01:11:34.037553Z","shell.execute_reply":"2025-03-30T01:11:34.036684Z","shell.execute_reply.started":"2025-03-30T01:11:21.992572Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.utils import class_weight\n","import numpy as np\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-25T15:08:36.568701Z","iopub.status.busy":"2025-03-25T15:08:36.568136Z","iopub.status.idle":"2025-03-25T15:08:37.156212Z","shell.execute_reply":"2025-03-25T15:08:37.155465Z","shell.execute_reply.started":"2025-03-25T15:08:36.568674Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Path to dataset files: /kaggle/input/inme-veri-seti-stroke-dataset\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"orvile/inme-veri-seti-stroke-dataset\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:11:45.697103Z","iopub.status.busy":"2025-03-30T01:11:45.696511Z","iopub.status.idle":"2025-03-30T01:11:46.340648Z","shell.execute_reply":"2025-03-30T01:11:46.339985Z","shell.execute_reply.started":"2025-03-30T01:11:45.697071Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}],"source":["#check gpu\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    print(\"GPU:\", gpus)\n","    for gpu in gpus:\n","        tf.config.experimental.set_memory_growth(gpu, True)\n","else:\n","    print(\"Cant find GPU, use CPU.\")\n","\n","from tensorflow.keras.mixed_precision import set_global_policy\n","set_global_policy('mixed_float16')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:11:50.817383Z","iopub.status.busy":"2025-03-30T01:11:50.817096Z","iopub.status.idle":"2025-03-30T01:11:50.821035Z","shell.execute_reply":"2025-03-30T01:11:50.820045Z","shell.execute_reply.started":"2025-03-30T01:11:50.817363Z"},"trusted":true},"outputs":[],"source":["data_dir = '/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:12:13.815608Z","iopub.status.busy":"2025-03-30T01:12:13.815326Z","iopub.status.idle":"2025-03-30T01:12:13.834412Z","shell.execute_reply":"2025-03-30T01:12:13.833634Z","shell.execute_reply.started":"2025-03-30T01:12:13.815587Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Count images: 2223\n","Count labels: 2223\n"]}],"source":["import glob\n","\n","hermorrhage_png = glob.glob(os.path.join(data_dir, 'Kanama/PNG/*.png'))\n","ischemia_png = glob.glob(os.path.join(data_dir, 'İskemi/PNG/*.png'))\n","\n","images = []\n","labels = []\n","\n","for img_path in hermorrhage_png:\n","    images.append(img_path)\n","    labels.append(1)  \n","\n","for img_path in ischemia_png:\n","    images.append(img_path)\n","    labels.append(1) \n","\n"," \n","\n","print(f\"Count images: {len(images)}\")\n","print(f\"Count labels: {len(labels)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Classification Model "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-03-25T15:25:00.515657Z","iopub.status.busy":"2025-03-25T15:25:00.515251Z","iopub.status.idle":"2025-03-25T15:25:00.957094Z","shell.execute_reply":"2025-03-25T15:25:00.956309Z","shell.execute_reply.started":"2025-03-25T15:25:00.515623Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Path to model files: /kaggle/input/resnet18-f37072fd_classification/pytorch/default/1\n"]}],"source":["# Download latest version\n","path = kagglehub.model_download(\"huongtraa/resnet18-f37072fd_classification/pyTorch/default\")\n","\n","print(\"Path to model files:\", path)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:12:31.097291Z","iopub.status.busy":"2025-03-30T01:12:31.097013Z","iopub.status.idle":"2025-03-30T01:15:15.878720Z","shell.execute_reply":"2025-03-30T01:15:15.877806Z","shell.execute_reply.started":"2025-03-30T01:12:31.097271Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","<ipython-input-7-8da236342496>:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"/kaggle/input/resnet18-f37072fd_classification/pytorch/default/1/resnet18-f37072fd.pth\"))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 0.4485, Accuracy: 79.87%\n","Epoch [2/10], Loss: 0.1872, Accuracy: 93.08%\n","Epoch [3/10], Loss: 0.1590, Accuracy: 93.87%\n","Epoch [4/10], Loss: 0.0995, Accuracy: 96.23%\n","Epoch [5/10], Loss: 0.0894, Accuracy: 96.79%\n","Epoch [6/10], Loss: 0.0732, Accuracy: 97.24%\n","Epoch [7/10], Loss: 0.0553, Accuracy: 98.09%\n","Epoch [8/10], Loss: 0.0448, Accuracy: 98.31%\n","Epoch [9/10], Loss: 0.0399, Accuracy: 98.71%\n","Epoch [10/10], Loss: 0.0326, Accuracy: 98.93%\n","Training complete!\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Define paths\n","base_path = \"/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/\"\n","ischemic_path = os.path.join(base_path, \"İskemi/PNG\")  # Updated to PNG\n","hemorrhagic_path = os.path.join(base_path, \"Kanama/PNG\")  # Updated to PNG\n","test_path = os.path.join(base_path, \"YarısmaVeriSeti_2_Oturum/PNG\")  # Updated to PNG\n","\n","# Custom Dataset for PNG images\n","class PNGDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        label = self.labels[idx]\n","        \n","        # Read PNG image\n","        img = Image.open(image_path).convert(\"RGB\")  # Ensure image is in RGB\n","        \n","        # Apply transformations if any (resize, normalize, etc.)\n","        if self.transform:\n","            img = self.transform(img)\n","        \n","        return img, label\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # ResNet-18 expects 224x224 images\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n","])\n","\n","# Prepare the data (for demonstration purposes, assuming binary labels: 0 for ischemic, 1 for hemorrhagic)\n","image_paths = []\n","labels = []\n","\n","# Add ischemic data (label 0 for ischemic)\n","for filename in os.listdir(ischemic_path):\n","    if filename.endswith(\".png\"):\n","        image_paths.append(os.path.join(ischemic_path, filename))\n","        labels.append(0)  # Ischemic -> 0\n","\n","# Add hemorrhagic data (label 1 for hemorrhagic)\n","for filename in os.listdir(hemorrhagic_path):\n","    if filename.endswith(\".png\"):\n","        image_paths.append(os.path.join(hemorrhagic_path, filename))\n","        labels.append(1)  # Hemorrhagic -> 1\n","\n","# Step 1: Split the data into train and validation sets (80% for train, 20% for validation)\n","train_files, val_files, train_labels, val_labels = train_test_split(\n","    image_paths, labels, test_size=0.2, random_state=42\n",")\n","\n","# Step 2: Create Dataset and DataLoader for training, validation, and test sets\n","train_dataset = PNGDataset(image_paths=train_files, labels=train_labels, transform=transform)\n","val_dataset = PNGDataset(image_paths=val_files, labels=val_labels, transform=transform)\n","test_files_all = [os.path.join(test_path, f) for f in os.listdir(test_path) if f.endswith(\".png\")]\n","test_labels_all = [1] * len(test_files_all)  # Dummy labels, replace with actual labels if available\n","test_dataset = PNGDataset(image_paths=test_files_all, labels=test_labels_all, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","# Step 1: Load ResNet-18 model without pre-trained weights\n","model = models.resnet18(pretrained=False)\n","\n","# Step 2: Load your custom pre-trained weights\n","model.load_state_dict(torch.load(\"/kaggle/input/resnet18-f37072fd_classification/pytorch/default/1/resnet18-f37072fd.pth\"))\n","\n","# Step 3: Modify the final fully connected layer for binary classification\n","model.fc = nn.Linear(model.fc.in_features, 2)\n","\n","# Step 4: Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Step 5: Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","\n","# Step 4: Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()  # Set the model to training mode\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    \n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Track loss and accuracy\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    \n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_accuracy = 100 * correct / total\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n","\n","print(\"Training complete!\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:15:32.094311Z","iopub.status.busy":"2025-03-30T01:15:32.093752Z","iopub.status.idle":"2025-03-30T01:15:37.899060Z","shell.execute_reply":"2025-03-30T01:15:37.898249Z","shell.execute_reply.started":"2025-03-30T01:15:32.094288Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.1962, Validation Accuracy: 94.83%\n"]}],"source":["# Step 5: Evaluate the model on the validation set\n","model.eval()  # Set the model to evaluation mode\n","val_correct = 0\n","val_total = 0\n","val_loss = 0.0\n","\n","with torch.no_grad():\n","    for inputs, labels in val_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","        \n","        _, predicted = torch.max(outputs, 1)\n","        val_total += labels.size(0)\n","        val_correct += (predicted == labels).sum().item()\n","\n","val_accuracy = 100 * val_correct / val_total\n","val_loss = val_loss / len(val_loader)\n","\n","print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:16:16.228926Z","iopub.status.busy":"2025-03-30T01:16:16.228604Z","iopub.status.idle":"2025-03-30T01:16:16.301598Z","shell.execute_reply":"2025-03-30T01:16:16.300530Z","shell.execute_reply.started":"2025-03-30T01:16:16.228903Z"},"trusted":true},"outputs":[],"source":["import os\n","os.makedirs(f\"/kaggle/working/models\", exist_ok=True)\n","\n","checkpoint_path = '/kaggle/working/models/classifier.pth'\n","\n","torch.save(model.state_dict(), checkpoint_path)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:22:52.795070Z","iopub.status.busy":"2025-03-30T01:22:52.794747Z","iopub.status.idle":"2025-03-30T01:22:56.079107Z","shell.execute_reply":"2025-03-30T01:22:56.078210Z","shell.execute_reply.started":"2025-03-30T01:22:52.795046Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Metrics:\n","Accuracy : 0.9483\n","Precision: 0.9845\n","Recall   : 0.9052\n","F1 Score : 0.9432\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","model.eval()\n","y_true = []\n","y_pred = []\n","\n","with torch.no_grad():\n","    for inputs, labels in val_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(preds.cpu().numpy())\n","\n","# Metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred)\n","\n","print(\"Validation Metrics:\")\n","print(f\"Accuracy : {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall   : {recall:.4f}\")\n","print(f\"F1 Score : {f1:.4f}\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:26:31.901165Z","iopub.status.busy":"2025-03-30T01:26:31.900854Z","iopub.status.idle":"2025-03-30T01:26:31.906700Z","shell.execute_reply":"2025-03-30T01:26:31.905935Z","shell.execute_reply.started":"2025-03-30T01:26:31.901142Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def demo_classifier_model(image_path, model, device, mask_dir=None, show=True):\n","    \"\"\"\n","    Demo classifier model on a single image.\n","    \n","    Args:\n","        image_path (str): Path to the test image (.png).\n","        model (torch.nn.Module): Trained classifier model.\n","        device (torch.device): CPU or GPU.\n","        mask_dir (str, optional): If provided, compares prediction with the ground truth mask.\n","        show (bool): Whether to display the image and mask.\n","    \"\"\"\n","    # Prepare image\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","    \n","    img = Image.open(image_path).convert(\"RGB\")\n","    input_tensor = transform(img).unsqueeze(0).to(device)\n","\n","    # Predict\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","        _, pred = torch.max(output, 1)\n","        pred_label = pred.item()\n","\n","    print(f\"\\n📷 Image: {os.path.basename(image_path)}\")\n","    print(f\"🧠 Predicted Label: {'Kanama (1)' if pred_label == 1 else 'İskemi (0)'}\")\n","\n","    "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:27:56.198758Z","iopub.status.busy":"2025-03-30T01:27:56.198408Z","iopub.status.idle":"2025-03-30T01:27:56.281617Z","shell.execute_reply":"2025-03-30T01:27:56.280928Z","shell.execute_reply.started":"2025-03-30T01:27:56.198729Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","📷 Image: 10147.png\n","🧠 Predicted Label: Kanama (1)\n"]}],"source":["image_path = '/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum/PNG/10147.png'\n","demo_classifier_model(image_path, model, device)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:37:22.421994Z","iopub.status.busy":"2025-03-30T01:37:22.421653Z","iopub.status.idle":"2025-03-30T01:37:22.427043Z","shell.execute_reply":"2025-03-30T01:37:22.426208Z","shell.execute_reply.started":"2025-03-30T01:37:22.421970Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","import os\n","\n","def show_image_pil(image_path):\n","    image = Image.open(image_path)\n","\n","    # Display the image\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.title(f'Displayed Image: {os.path.basename(image_path)}')\n","\n","    # Create legend handles\n","    legend_patches = [\n","        mpatches.Patch(color='green', label='Kanama'),\n","        mpatches.Patch(color='blue', label='Iskemi')\n","    ]\n","    plt.legend(handles=legend_patches, loc='upper right', frameon=False, labelcolor='white')\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2025-03-30T01:37:26.828284Z","iopub.status.busy":"2025-03-30T01:37:26.827985Z","iopub.status.idle":"2025-03-30T01:37:26.947548Z","shell.execute_reply":"2025-03-30T01:37:26.946944Z","shell.execute_reply.started":"2025-03-30T01:37:26.828260Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["mask_path='/kaggle/input/inme-veri-seti-stroke-dataset/İNME VERİ SETİ/YarısmaVeriSeti_2_Oturum/MASKS/10147.png'\n","show_image_pil(mask_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6652053,"sourceId":10729545,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":278845,"modelInstanceId":257549,"sourceId":301568,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
